{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables as tb\n",
    "import torch\n",
    "from torch_geometric.utils import segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.14\n",
      "IPython version      : 8.24.0\n",
      "\n",
      "torch          : 2.2.2\n",
      "torch_geometric: 2.5.2\n",
      "torch_scatter  : 2.1.2\n",
      "tables         : 3.9.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p torch,torch_geometric,torch_scatter,tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7182220, 320])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = \"datasets/protein_embeddings/test_set_esm-small_inputs.graphfmt.h5\"\n",
    "\n",
    "# to use torch_geometric.uitls.segment, these need to be torch tensors and NOT numpy arrays\n",
    "with tb.open_file(data_file) as fp:\n",
    "    ptn_embed = torch.from_numpy(fp.root.data[:])\n",
    "    genome_ptr = torch.from_numpy(fp.root.ptr[:])\n",
    "\n",
    "\n",
    "# the ptn embeddings are stored in a stacked batch\n",
    "# this is a 2D tensor of shape (num proteins, embed_dim)\n",
    "ptn_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([151256]),\n",
       " tensor([      0,      24,      36,  ..., 7181373, 7181835, 7182220]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to keep track of where the proteins are for a given genome, we use the genome_ptr\n",
    "# the genome_ptr is a 1D tensor of shape (num genomes + 1)\n",
    "# the first element is always 0, and the last element is the total number of proteins\n",
    "# this storage is basically to the CSR format used in sparse matrices\n",
    "# see PyTorch-Geometric\n",
    "genome_ptr.shape, genome_ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about this data handling and batching procedure as described by PyTorch-Geometric [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 24,  12,  11,  ..., 399, 462, 385])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get the number of proteins encoded by each genome like this:\n",
    "genome_ptr[1:] - genome_ptr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0280, -0.2302,  0.2222,  ...,  0.0936,  0.0269, -0.1448],\n",
       "         [-0.0183, -0.2730,  0.3387,  ...,  0.0386,  0.1384,  0.0912],\n",
       "         [-0.0685, -0.3271,  0.0728,  ...,  0.2096,  0.1714, -0.0694],\n",
       "         [-0.2642, -0.0469,  0.0405,  ...,  0.0922, -0.2237, -0.0227]]),\n",
       " torch.Size([4, 320]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 8 protein embeddings for the 12346th genome can be retrieved like this:\n",
    "genome_idx = 12345\n",
    "start = genome_ptr[genome_idx]\n",
    "end = genome_ptr[genome_idx + 1]\n",
    "data = ptn_embed[start:end]\n",
    "data, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0303, -0.3255,  0.2116,  ...,  0.0552,  0.2166,  0.0266],\n",
       "         [-0.0071, -0.2595,  0.2108,  ...,  0.0519,  0.2123,  0.0655],\n",
       "         [ 0.0369, -0.2487,  0.2112,  ...,  0.0966,  0.1809,  0.0249],\n",
       "         ...,\n",
       "         [ 0.2230, -0.0103,  0.0860,  ...,  0.1798, -0.2928, -0.0601],\n",
       "         [ 0.2105, -0.0035,  0.0710,  ...,  0.1934, -0.3043, -0.0727],\n",
       "         [ 0.1954,  0.0189,  0.0122,  ...,  0.2291, -0.3480, -0.1056]]),\n",
       " torch.Size([151255, 320]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use pytorch-geometric and pytorch-scatter to perform reductions on this stacked batch format using the genome_ptr\n",
    "# to average the protein embeddings per genome:\n",
    "genome_embed = segment(src=ptn_embed, ptr=genome_ptr, reduce=\"mean\")\n",
    "genome_embed, genome_embed.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
