{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO79PGfkxK7uUGf+a1TeYdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a941ca8a05248e1a6d32c956c4144a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a93bf7750849a39b28527f364ea5af",
              "IPY_MODEL_d4371b2864a34e3ba61c404edad2ccec",
              "IPY_MODEL_da6d54dd32a04bc4bd2818ab01940748"
            ],
            "layout": "IPY_MODEL_180aea0bf26a48c682245092d23f5f43"
          }
        },
        "62a93bf7750849a39b28527f364ea5af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731aa4dd27044b64b57f494a4be1bb50",
            "placeholder": "​",
            "style": "IPY_MODEL_6ae5ed1f520943c3885fa4b6516e976e",
            "value": "Predicting DataLoader 0: 100%"
          }
        },
        "d4371b2864a34e3ba61c404edad2ccec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51b054068a88459eae3f1eca03db5605",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_321e7f52473544e5ac67dbc9cf673bb2",
            "value": 1
          }
        },
        "da6d54dd32a04bc4bd2818ab01940748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c66f247e8aac430c8a6d09fc8ace3927",
            "placeholder": "​",
            "style": "IPY_MODEL_468db8f6bf014962a91cfb8c278d12da",
            "value": " 511/511 [05:30&lt;00:00,  1.55it/s]"
          }
        },
        "180aea0bf26a48c682245092d23f5f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "731aa4dd27044b64b57f494a4be1bb50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae5ed1f520943c3885fa4b6516e976e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51b054068a88459eae3f1eca03db5605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321e7f52473544e5ac67dbc9cf673bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c66f247e8aac430c8a6d09fc8ace3927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468db8f6bf014962a91cfb8c278d12da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cody-mar10/protein_set_transformer/blob/main/examples/pst_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PST Inference\n",
        "\n",
        "## 1. GPU Runtime\n",
        "Ensure that you are using a runtime with GPU access:\n",
        "`Runtime > Change runtime type` and choose a GPU runtime.\n",
        "\n",
        "## 2. Setup software\n",
        "Google Colab servers already have the latest version of `PyTorch`. We need to check the version of `PyTorch` and `CUDA` to properly install other `PyTorch` extension libraries required by `PST`.\n",
        "\n",
        "All installation should take less than 1 minute."
      ],
      "metadata": {
        "id": "Nxr96eqDeB3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "goEZyafiLCW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o56j4cCTLGDt",
        "outputId": "8c4f1f54-c84e-410d-a69a-c70e69d95b3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wheel URL should be in the form: `https://data.pyg.org/whl/torch-{TORCHVERSION}+{CUDA}.html`\n",
        "\n",
        "Just change the command below to use the correct version information."
      ],
      "metadata": {
        "id": "cWBU5AZWeszU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install torch-geometric torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.8.0+cu126.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4E0vkdCREju",
        "outputId": "3f4e2ffc-2a7d-4f0a-dd43-abc975c81125"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m25 packages\u001b[0m \u001b[2min 897ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 675ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-geometric\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-scatter\u001b[0m\u001b[2m==2.1.2+pt28cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch-sparse\u001b[0m\u001b[2m==0.6.18+pt28cu126\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then install the `PST` library. *Note: must be `>=2.6.0` since that minor release unlocks the max python and PyTorch versions*"
      ],
      "metadata": {
        "id": "DA0Utbove-vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install \"ptn-set-transformer>=2.6.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIDgYhsiLHap",
        "outputId": "2b1bbbb0-f09d-469e-af90-fc72b8dcf380"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m84 packages\u001b[0m \u001b[2min 564ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m13 packages\u001b[0m \u001b[2min 327ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m13 packages\u001b[0m \u001b[2min 82ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mboltons\u001b[0m\u001b[2m==25.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcattrs\u001b[0m\u001b[2m==25.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfair-esm\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonargparse\u001b[0m\u001b[2m==4.41.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning\u001b[0m\u001b[2m==2.5.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning-cv\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlightning-utilities\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moptuna\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mptn-set-transformer\u001b[0m\u001b[2m==2.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytorch-lightning\u001b[0m\u001b[2m==2.5.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchmetrics\u001b[0m\u001b[2m==1.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtypeshed-client\u001b[0m\u001b[2m==2.8.2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mount Google Drive (optional)\n",
        "PST inference requires specially formatted HDF5 files that can be created and stored on your Google Drive account.\n",
        "\n",
        "Your Google Drive account can be mounted to this runtime server so that your files are accessible.\n",
        "\n",
        "You can store your data files there (and models if you want, but those can also be downloaded locally).\n",
        "\n",
        "-----\n",
        "\n",
        "Mounting your Google Drive will prompt authentication each time this notebook is ran:"
      ],
      "metadata": {
        "id": "G3jedvqef689"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvkV824NgfHc",
        "outputId": "d539a102-17fe-408c-ec5f-d6670554f81c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After mounting your Google Drive, your files should be accessible at the path `/content/drive/MyDrive`.\n",
        "\n",
        "For example, I have a demo folder that has a test FASTA file (which is the first 250 scaffolds encoding 8,955 proteins from the PST training set)."
      ],
      "metadata": {
        "id": "wpKUW9KGhG_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/pst_demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIR1i3_6g_nO",
        "outputId": "bf86914f-a720-45c7-a948-485705f441e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 30M\n",
            "-rw------- 1 root root  27M Oct  1 16:01 PST_embeddings.h5\n",
            "-rw------- 1 root root 3.4M Oct  1 16:12 test.faa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Compute ESM2 embeddings (optional)\n",
        "\n",
        "If you already have ESM2 embeddings for your protein sequences, then you can skip this step.\n",
        "\n",
        "-----\n",
        "\n",
        "**Advanced**: If you have a *large* number of proteins (ie 1M+), then you could benefit from splitting your large FASTA file into smaller segments (~100k sequences) to embed independently. This obviously benefits more if you have access to multiple GPUs to split the work across, but this will also help if you need to restart this runtime due to timelimits.\n",
        "\n",
        "You will need to concatenate the embeddings back **in the same order** since the FASTA file should be sorted such that the proteins are in order for each scaffold (based on the position in the genome)."
      ],
      "metadata": {
        "id": "hx-E4EhapSaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pst.embed.model import ESM2Models\n",
        "def esm_embed(file: str, esm_model: ESM2Models = ESM2Models.esm2_t30_150M, batch_size: int = 2048, outdir: Path = Path(\".\")):\n",
        "    \"\"\"\n",
        "    Compute ESM2 embeddings for a FASTA file.\n",
        "\n",
        "    Args:\n",
        "        file (str): Path to FASTA file.\n",
        "        esm_model (ESM2Models, optional): ESM2 model to use. Defaults to ESM2Models.esm2_t30_150M.\n",
        "        batch_size (int, optional): Batch size in number of amino acids. Defaults to 2048.\n",
        "    \"\"\"\n",
        "    from pst.embed import ModelArgs, TrainerArgs, embed\n",
        "\n",
        "    model_args = ModelArgs(esm=esm_model, batch_size=batch_size)\n",
        "    trainer_args = TrainerArgs()\n",
        "\n",
        "    embed(input=Path(file), outdir=outdir, model_cfg=model_args, trainer_cfg=trainer_args)"
      ],
      "metadata": {
        "id": "nyWfYhIxjjO-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will embed the sequences to `{outdir}/{esm2_model_name}_results.h5`\n",
        "\n",
        "This will download the relevant ESM2 model, but that could also be uploaded to your Google Drive. This would require you to adjust your `$TORCH_HOME` environment variable appropriately."
      ],
      "metadata": {
        "id": "O5So_nqI2cjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasta_file = \"/content/drive/MyDrive/pst_demo/test.faa\""
      ],
      "metadata": {
        "id": "OwINrpT3ZkGY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the length of the proteins in your FASTA file, you may need to lower the `batch_size` (which is in units of amino acids).\n",
        "\n",
        "This will by far take the most amount of time in the PST inference process (~1 min/2.5k proteins). If you have access to more powerful GPUs or can upgrade a free Colab notebook, you would greatly benefit from ESM2 inference needed for PST **if you have a large number of proteins**."
      ],
      "metadata": {
        "id": "YO97Ftd2ip6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "esm_embed(fasta_file, batch_size=4096)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "4a941ca8a05248e1a6d32c956c4144a8",
            "62a93bf7750849a39b28527f364ea5af",
            "d4371b2864a34e3ba61c404edad2ccec",
            "da6d54dd32a04bc4bd2818ab01940748",
            "180aea0bf26a48c682245092d23f5f43",
            "731aa4dd27044b64b57f494a4be1bb50",
            "6ae5ed1f520943c3885fa4b6516e976e",
            "51b054068a88459eae3f1eca03db5605",
            "321e7f52473544e5ac67dbc9cf673bb2",
            "c66f247e8aac430c8a6d09fc8ace3927",
            "468db8f6bf014962a91cfb8c278d12da"
          ]
        },
        "id": "0rtKUvZnzpYc",
        "outputId": "2d9140c6-c260-44ef-b5e7-e5285543c935"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 111\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t30_150M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t30_150M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t30_150M_UR50D-contact-regression.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Predicting: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a941ca8a05248e1a6d32c956c4144a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4DT5VmaYfY3",
        "outputId": "59cbdc81-e792-41ca-d2e2-a93507157f60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 21M Oct  1 16:19 esm2_t30_150M_results.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Graph format embeddings\n",
        "The embedding file needs to be reformatted to a graph format used by PST.\n",
        "\n",
        "For protein FASTA files generated by prodigal/pyrodigal, the following should be sufficient. However, `OptionalArgs` also takes an optional strand mapping file that maps each protein to a strand `[-1, 1]` (in a tab-delimited format: `protein\\tstrand`.\n",
        "\n",
        "The `scaffold_map_file`maps scaffolds to genomes for multi-scaffold genomes in a tab-delimited format: `scaffold name\\tgenome name`. `scaffold name` is defined as the part of the protein name before the numerical identifier: `scaffold name_PTNID`\n",
        "\n",
        "This will create a new file `{outdir}/{esm2_model_name}_results.graphfmt.h5` that should be used as input to `PST` models."
      ],
      "metadata": {
        "id": "N05JOBgo2rsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graphify(embeddings_file: str, fasta_file: str, scaffold_map_file: str | None = None):\n",
        "    from pst.utils.graphify import IOArgs, OptionalArgs, to_graph_format\n",
        "\n",
        "    io_args = IOArgs(file=Path(embeddings_file), fasta_file=Path(fasta_file))\n",
        "    optional_args = OptionalArgs(scaffold_map_file=Path(scaffold_map_file) if scaffold_map_file is not None else None)\n",
        "\n",
        "    to_graph_format(io_args, optional_args)"
      ],
      "metadata": {
        "id": "_SiTBg8sW69R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphify(embeddings_file=\"esm2_t30_150M_results.h5\", fasta_file=fasta_file)"
      ],
      "metadata": {
        "id": "d8MFzg7bZaqZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dZh8XKHdOou",
        "outputId": "ca864ab0-9930-415c-978e-4aa9e99da976"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 21M Oct  1 16:19 esm2_t30_150M_results.graphfmt.h5\n",
            "-rw-r--r-- 1 root root 21M Oct  1 16:19 esm2_t30_150M_results.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are now extra fields in the HDF5 file that enable efficient access to all protein embeddings from each genome."
      ],
      "metadata": {
        "id": "sBLCMUkNdSEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tables as tb"
      ],
      "metadata": {
        "id": "NiB7iBtAdZNP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "esm_embeddings_file = \"esm2_t30_150M_results.graphfmt.h5\"\n",
        "with tb.open_file(esm_embeddings_file) as fp:\n",
        "    for node in fp.walk_nodes(classname=\"Array\"):\n",
        "        print(node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFwr5_OCda9r",
        "outputId": "835ac97a-f410-4377-e950-38be6c75481c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/data (CArray(np.int64(8955), np.int64(640))shuffle, blosc:lz4(4)) ''\n",
            "/ptr (CArray(np.int64(251),)shuffle, blosc:lz4(4)) ''\n",
            "/sizes (CArray(np.int64(250),)shuffle, blosc:lz4(4)) ''\n",
            "/strand (CArray(np.int64(8955),)shuffle, blosc:lz4(4)) ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. PST inference\n",
        "With graph-formatted ESM2 embeddings, `PST` can be used for inference.\n",
        "\n",
        "Only 2 different ESM2 embedding sizes were used to train PSTs:\n",
        "- `esm2_t6_8M` -> `PST...__small`\n",
        "- `esm2_t30_150M` -> `PST...__large`\n",
        "\n",
        "So the correct `PST` model needs to be chosen based on the ESM2 embeddings generated.\n",
        "\n",
        "There are also several PST models trained with different objectives and cross validation strategies:\n",
        "- `PST-TL-P__small`\n",
        "- `PST-TL-P__large`\n",
        "- `PST-TL-T__small`\n",
        "- `PST-TL-T__large`\n",
        "- `PST-MLM-P__small`\n",
        "- `PST-MLM-P__large`\n",
        "- `PST-MLM-T__small`\n",
        "- `PST-MLM-T__large`\n",
        "\n",
        "I recommend starting with `PST-TL-P` models.\n",
        "\n",
        "### 5.1 Download PST model\n",
        "\n",
        "The following code uses the `PST` API to download the models from the DRYAD repository [https://doi.org/10.5061/dryad.d7wm37q8w](https://doi.org/10.5061/dryad.d7wm37q8w).\n",
        "\n",
        "For simplicity, the model checkpoint is downloaded to the current directory (`/content/drive/`)"
      ],
      "metadata": {
        "id": "d8v28F5ddorV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choices:\n",
        "# [\n",
        "#     \"PST-TL-P__small\",\n",
        "#     \"PST-TL-P__large\",\n",
        "#     \"PST-TL-T__small\",\n",
        "#     \"PST-TL-T__large\",\n",
        "#     \"PST-MLM\", # <- NOTE: this will create a subdirectory, so keep that in mind if using\n",
        "# ]\n",
        "def download_model(model: str | list[str]):\n",
        "    from pst.utils.download import DryadDownloader\n",
        "    from pst.utils.cli.download import ManuscriptDataArgs, ClusterArgs, EmbeddingsArgs, ModelArgs\n",
        "\n",
        "    if isinstance(model, str):\n",
        "        model = [model]\n",
        "\n",
        "    downloader = DryadDownloader(\n",
        "        manuscript=ManuscriptDataArgs(),\n",
        "        cluster=ClusterArgs(),\n",
        "        embeddings=EmbeddingsArgs(),\n",
        "        model=ModelArgs(choices=model),\n",
        "        all=False,\n",
        "        outdir=Path(\".\")\n",
        "    )\n",
        "\n",
        "    downloader.download()\n",
        "\n",
        "download_model(\"PST-TL-P__large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFFjWkkOfFBR",
        "outputId": "dbb0198d-1823-4af6-b94f-800088e8c1d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the following 2 files to .\n",
            "\tREADME.md\n",
            "\tPST-TL-P__large.ckpt.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[0/2] README.md: 100%|██████████| 28.7k/28.7k [00:00<00:00, 915kB/s]\n",
            "[1/2] PST-TL-P__large.ckpt.gz: 100%|██████████| 221M/221M [00:03<00:00, 70.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/2] Download finished.\n",
            "Decompressing all tarballs, zip files, and gzipped files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YykEQYKJe_OI",
        "outputId": "7de0b89b-ab9f-49be-e88c-84b9813259f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 286M\n",
            "drwx------ 6 root root 4.0K Oct  1 16:13 drive\n",
            "-rw-r--r-- 1 root root  21M Oct  1 16:19 esm2_t30_150M_results.graphfmt.h5\n",
            "-rw-r--r-- 1 root root  21M Oct  1 16:19 esm2_t30_150M_results.h5\n",
            "-rw-r--r-- 1 root root 245M Oct  1 16:19 PST-TL-P__large.ckpt\n",
            "-rw-r--r-- 1 root root  29K Oct  1 16:19 README.md\n",
            "drwxr-xr-x 1 root root 4.0K Sep 29 13:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the `model_inference` function to generate PST embeddings."
      ],
      "metadata": {
        "id": "roEoFZ6ugzO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pst\n",
        "from pst.predict import model_inference\n",
        "from pst.predict.predict import PredictArgs, AcceleratorOpts"
      ],
      "metadata": {
        "id": "vgbx9bs8LW7Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The `output` file should be saved to your Google Drive account so that it is permanent. Otherwise, it will be deleted if stored locally on this server."
      ],
      "metadata": {
        "id": "j_3RY80ug3iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "esm_embeddings_file = Path(\"esm2_t30_150M_results.graphfmt.h5\")\n",
        "model_checkpoint = Path(\"PST-TL-P__large.ckpt\") # CHANGE TO MODEL DOWNLOADED\n",
        "\n",
        " # NOTE: you will want this to be saved to your Google Drive so it will persist after this notebook ends\n",
        "output = Path(\"/content/drive/MyDrive/pst_demo/PST_embeddings.h5\")\n",
        "\n",
        "results = model_inference(\n",
        "    model_type=pst.ProteinSetTransformer,\n",
        "    file=esm_embeddings_file,\n",
        "    predict=PredictArgs(checkpoint=model_checkpoint, output=output),\n",
        "    lazy=True,\n",
        "    accelerator=AcceleratorOpts.gpu,\n",
        "\n",
        "    # OPTIONAL: if you want inspect results in this notebook\n",
        "    # The results are always saved to the file above\n",
        "    return_predictions=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnr21Dh5hob2",
        "outputId": "a402ed05-018d-4898-cf20-a52f3ed61a7a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pst.predict.predict:Output file /content/drive/MyDrive/pst_demo/PST_embeddings.h5 already exists and will be overwritten\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00,  9.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purposes of this notebook, I returned the predictions to inspect.\n",
        "\n",
        "Notice the shape of the first (batch) dimension of each tensor:\n",
        "- `protein` and `attn` are the same as the number of proteins\n",
        "- `genome` is the same as the number of scaffolds in this case\n",
        "  - Note: all the genomes in the test file were single scaffold, so `genome` and `scaffold` are interchangeable.\n",
        "  - For datasets that include multi-scaffold genomes, there will also be a `scaffold` field **if the graph-formatted HDF5 file includes a `scaffold_label` field that maps each scaffold to a unique integer corresponding to the genome**."
      ],
      "metadata": {
        "id": "wbrvYdTRgxtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in results.items():\n",
        "    print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg3zPRBVh2Ac",
        "outputId": "a2a3c6c1-875c-4c10-faa0-9f1eff35b59a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "protein torch.Size([8955, 800])\n",
            "attn torch.Size([8955, 4])\n",
            "genome torch.Size([250, 800])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/pst_demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1esZtV1_iSLd",
        "outputId": "2e145291-67d1-4c62-f0c0-b6771b5915b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 30M\n",
            "-rw------- 1 root root  27M Oct  1 16:19 PST_embeddings.h5\n",
            "-rw------- 1 root root 3.4M Oct  1 16:12 test.faa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can inspect the HDF5 file to see that the fields present in the file are identical to the returned results."
      ],
      "metadata": {
        "id": "UfRGevfvh-Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tb.open_file(output) as fp:\n",
        "    for node in fp.walk_nodes(classname=\"Array\"):\n",
        "        print(node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Hxp_Cbh8WQ",
        "outputId": "2f9cecc7-0bbc-4a8b-d1b6-2e1ac5e28628"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/attn (EArray(np.int64(8955), np.int64(4))shuffle, blosc:lz4(4)) ''\n",
            "/ctx_ptn (EArray(np.int64(8955), np.int64(800))shuffle, blosc:lz4(4)) ''\n",
            "/genome (EArray(np.int64(250), np.int64(800))shuffle, blosc:lz4(4)) ''\n"
          ]
        }
      ]
    }
  ]
}